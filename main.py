# main.py
from scraper import scrape_questions
from save_to_file import save_to_word
from save_json import save_to_json, load_from_json
import json
import os


def remove_duplicates(questions):
    """å»é‡ï¼Œç¡®ä¿ç›¸åŒé¢˜ç›®ä¸é‡å¤"""
    unique_questions = []
    seen_questions = set()

    for question in questions:
        question_text_image = question["question"] + "_" + question["image"]
        if question_text_image not in seen_questions:
            seen_questions.add(question_text_image)
            unique_questions.append(question)

    return unique_questions


def get_question_set(questions):
        
    # åˆ›å»ºä¸€ä¸ªç©ºçš„ set
    question_set = set()

    # éå† data_listï¼Œå°† question + "_" + image ç»„åˆååŠ å…¥ set
    for item in questions:
        question_set.add(item["question"] + "_" + item["image"])
    
    return question_set


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æŠ“å– ICBC é¢˜ç›®...")

    all_questions = []  # å­˜å‚¨æ‰€æœ‰å»é‡åçš„é¢˜ç›®

    # å¦‚æœå­˜åœ¨å·²ç»ä¿å­˜çš„ JSON æ–‡ä»¶ï¼Œå¯ä»¥é€‰æ‹©è¯»å–æ–‡ä»¶ä¸­çš„é¢˜ç›®
    all_questions = load_from_json()  # å°è¯•ä» JSON æ–‡ä»¶åŠ è½½é¢˜ç›®


    # # è¿™éƒ¨åˆ†ä¸ºè¡¥å……ï¼ŒåŠ è½½æœ€è¿‘ä¸¤ä¸ªç‰ˆæœ¬ start
    # json_filename = os.path.join("json", "questions_20250322.json")
    # all_questions2 = load_from_json(json_filename)  # å°è¯•ä» JSON æ–‡ä»¶åŠ è½½é¢˜ç›®
    # all_questions.extend(all_questions2)  # åˆå¹¶é¢˜ç›®
    # all_questions = remove_duplicates(all_questions)  # å»é‡
    # print(f"âœ… ä» Json æ–‡ä»¶åŠ è½½å®Œæ¯•, å»é‡åï¼Œå½“å‰æ€»é¢˜ç›®æ•°: {len(all_questions2)}")
    # # è¿™éƒ¨åˆ†ä¸ºè¡¥å……  end

    question_set = get_question_set(all_questions)

    # è¿›è¡ŒæŠ“å–  å®Œæ•´æµ‹è¯•
    for i in range(50):  # æ§åˆ¶æŠ“å–æ¬¡æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        print(f"\n========================================================================")
        print(f"ğŸ”„ ç¬¬ {i+1} æ¬¡æŠ“å–..å®Œæ•´æµ‹è¯•...")
        new_questions = scrape_questions(True, question_set)

        if new_questions:
            all_questions.extend(new_questions)  # åˆå¹¶é¢˜ç›®
            all_questions = remove_duplicates(all_questions)  # å»é‡
            question_set = get_question_set(all_questions)
            print(f"âœ… å½“å‰æ€»é¢˜ç›®æ•°: {len(all_questions)}")

            # ç»§ç»­å¤„ç†é¢˜ç›®ï¼Œä¿å­˜åˆ° Word
            print(f"âœ… å…¨éƒ¨æŠ“å–å®Œæˆï¼Œå…± {len(all_questions)} é“é¢˜ç›®")
            save_to_word(all_questions)  # ä¿å­˜åˆ° Word
            # å­˜å‚¨æŠ“å–åˆ°çš„é¢˜ç›®ä¸º JSON æ–‡ä»¶
            save_to_json(all_questions)  # ä¿å­˜åˆ° JSON æ–‡ä»¶

        else:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡æŠ“å– æ²¡æœ‰æ–°é¢˜ç›®ï¼Œè·³è¿‡")

    
    # è¿›è¡ŒæŠ“å–  æ ‡å¿—æµ‹è¯•
    for i in range(50):  # æ§åˆ¶æŠ“å–æ¬¡æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        print(f"\n========================================================================")
        print(f"ğŸ”„ ç¬¬ {i+1} æ¬¡æŠ“å–..æ ‡å¿—æµ‹è¯•...")
        new_questions = scrape_questions(False, question_set)

        if new_questions:
            all_questions.extend(new_questions)  # åˆå¹¶é¢˜ç›®
            all_questions = remove_duplicates(all_questions)  # å»é‡
            question_set = get_question_set(all_questions)
            print(f"âœ… å½“å‰æ€»é¢˜ç›®æ•°: {len(all_questions)}")

            # ç»§ç»­å¤„ç†é¢˜ç›®ï¼Œä¿å­˜åˆ° Word
            print(f"âœ… å…¨éƒ¨æŠ“å–å®Œæˆï¼Œå…± {len(all_questions)} é“é¢˜ç›®")
            save_to_word(all_questions)  # ä¿å­˜åˆ° Word
            # å­˜å‚¨æŠ“å–åˆ°çš„é¢˜ç›®ä¸º JSON æ–‡ä»¶
            save_to_json(all_questions)  # ä¿å­˜åˆ° JSON æ–‡ä»¶

        else:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡æŠ“å– æ²¡æœ‰æ–°é¢˜ç›®ï¼Œè·³è¿‡")
    

    # å­˜å‚¨æŠ“å–åˆ°çš„é¢˜ç›®ä¸º JSON æ–‡ä»¶
    if all_questions:
        print(f"âœ… å…¨éƒ¨æŠ“å–å®Œæˆï¼Œå…± {len(all_questions)} é“é¢˜ç›®")
        save_to_json(all_questions)  # ä¿å­˜åˆ° JSON æ–‡ä»¶
        # ç»§ç»­å¤„ç†é¢˜ç›®ï¼Œä¿å­˜åˆ° Word
        save_to_word(all_questions)  # ä¿å­˜åˆ° Word
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•é¢˜ç›®")